import torch
import torch.nn.functional as F
from tqdm import tqdm
import higher
import numpy as np
import copy

def meta_train_one_epoch(meta_learner, dataloader, meta_optimizer, device, grad_clip_norm):
    """
    Ìïú ÏóêÌè≠Ïùò Î©îÌÉÄ-ÌïôÏäµÏùÑ ÏàòÌñâÌïòÍ≥† ÌèâÍ∑† outer lossÎ•º Î∞òÌôò.
    - higherÎ°ú meta_learner.prior_netÏùÑ inner-loop Ï†ÅÏùë
    - NaN/Inf Í∞ÄÎìú Î∞è gradient clipping Ï†ÅÏö©
    - AMP(ÏÑ†ÌÉù) ÏßÄÏõê: CONFIG['USE_AMP']=True Ïãú ÏûêÎèô ÌôúÏÑ±Ìôî
    - üîß ÏûÖÎ†• ÌòïÌÉú [1, K, C, H, W] -> [B=K, C, H, W]Î°ú ÌèâÌÉÑÌôî Ï≤òÎ¶¨
    """
    import torch
    import higher
    from torch.amp import autocast, GradScaler  # ÏµúÏã† Í∂åÏû• API

    def _flatten_task_batch(x, is_target=False):
        """
        x: [1, K, C, H, W] -> [K, C, H, W]
        (Î°úÎçî batch_size=1 Ï†ÑÏ†ú. ÎßåÏïΩ [K, C, H, W]Î©¥ Í∑∏ÎåÄÎ°ú ÌÜµÍ≥º)
        """
        if x.dim() == 5:
            b, k, c, h, w = x.shape
            assert b == 1, f"Expected loader batch=1, got {b}"
            return x.view(k, c, h, w)
        elif x.dim() == 4:
            return x
        else:
            raise RuntimeError(f"Unexpected tensor shape {tuple(x.shape)} for {'target' if is_target else 'input'}")

    meta_learner.train()
    cfg = meta_learner.config

    # ÏÑ§Ï†ï ÌÇ§ Ìò∏Ìôò (INNER_STEPS vs NUM_ADAPTATION_STEPS)
    inner_steps = int(cfg.get("NUM_ADAPTATION_STEPS", cfg.get("INNER_STEPS", 5)))
    inner_lr    = float(cfg.get("INNER_LR", 1e-3))
    use_amp     = bool(cfg.get("USE_AMP", False))

    scaler = GradScaler('cuda', enabled=use_amp)

    running_outer = 0.0
    n_tasks = 0

    meta_optimizer.zero_grad(set_to_none=True)

    for batch in tqdm(dataloader, desc="Meta-Training"):
        if len(batch) != 4:
            raise ValueError("Each batch must be (support_x, support_y, query_x, query_y).")
        support_x, support_y, query_x, query_y = batch

        # ÎîîÎ∞îÏù¥Ïä§ Ïù¥Îèô
        support_x = support_x.to(device, non_blocking=True)
        support_y = support_y.to(device, non_blocking=True)
        query_x   = query_x.to(device, non_blocking=True)
        query_y   = query_y.to(device, non_blocking=True)

        # üîß [1, K, C, H, W] -> [K, C, H, W] ÌèâÌÉÑÌôî
        support_x = _flatten_task_batch(support_x, is_target=False).contiguous()
        support_y = _flatten_task_batch(support_y, is_target=True).contiguous()
        query_x   = _flatten_task_batch(query_x,   is_target=False).contiguous()
        query_y   = _flatten_task_batch(query_y,   is_target=True).contiguous()

        # inner optimizer (taskÎ≥Ñ Ï¥àÍ∏∞Ìôî)
        base_params = list(meta_learner.prior_net.parameters())
        inner_opt = torch.optim.SGD(base_params, lr=inner_lr, momentum=0.0)

        # higher Ïª®ÌÖçÏä§Ìä∏: prior_netÏùÑ Ï†ÅÏùë ÎåÄÏÉÅÏúºÎ°ú ÏÇ¨Ïö©
        with higher.innerloop_ctx(
            meta_learner.prior_net,
            inner_opt,
            copy_initial_weights=True,     # ÌÉúÏä§ÌÅ¨ÎßàÎã§ Íπ®ÎÅóÌïú Ï¥àÍ∏∞ ÌååÎùºÎØ∏ÌÑ∞
            track_higher_grads=True        # Î©îÌÉÄ ÏóÖÎç∞Ïù¥Ìä∏ ÏúÑÌï¥ Í≥†Í≥Ñ ÎØ∏Î∂Ñ Ï∂îÏ†Å
        ) as (fmodel, diffopt):

            # ---------- Inner loop (support) ----------
            for _ in range(inner_steps):
                with autocast('cuda', enabled=use_amp):
                    inner_loss = meta_learner.inner_loop_loss(fmodel, support_x, support_y)
                    inner_loss = torch.nan_to_num(inner_loss, nan=0.0, posinf=1e6, neginf=1e6)

                # higherÍ∞Ä ÎÇ¥Î∂ÄÏóêÏÑú backward+update Ï≤òÎ¶¨. innerÏóêÏÑúÎäî .gradÎ•º Í±¥ÎìúÎ¶¨ÏßÄ ÏïäÏùå
                diffopt.step(inner_loss)

            # ---------- Outer loss (query) ----------
            with autocast('cuda', enabled=use_amp):
                outer_loss = meta_learner.outer_loop_loss(fmodel, query_x, query_y)
                outer_loss = torch.nan_to_num(outer_loss, nan=0.0, posinf=1e6, neginf=1e6)

            # Î©îÌÉÄ Í∑∏ÎùºÎìú ÎàÑÏ†Å (ÏõêÎ≥∏ prior_net ÌååÎùºÎØ∏ÌÑ∞Î°ú)
            if use_amp:
                scaler.scale(outer_loss).backward()
            else:
                outer_loss.backward()

            running_outer += float(outer_loss.detach().cpu())
            n_tasks += 1

        # ---------- Meta step (taskÎßàÎã§ ÏóÖÎç∞Ïù¥Ìä∏) ----------
        # Î©îÌÉÄ Í∑∏ÎùºÎìú Í∞ÄÎìú + ÌÅ¥Î¶Ω
        for p in meta_learner.prior_net.parameters():
            if p.grad is not None:
                p.grad.data = torch.nan_to_num(p.grad.data, nan=0.0, posinf=0.0, neginf=0.0)

        if grad_clip_norm is not None and grad_clip_norm > 0:
            torch.nn.utils.clip_grad_norm_(meta_learner.prior_net.parameters(), max_norm=grad_clip_norm)

        if use_amp:
            scaler.step(meta_optimizer)
            scaler.update()
        else:
            meta_optimizer.step()

        meta_optimizer.zero_grad(set_to_none=True)

    avg_outer = running_outer / max(1, n_tasks)
    return avg_outer




def meta_evaluate(meta_learner, test_task, device, num_adaptation_steps, num_eval_samples, debug=False):
    """
    ÌèâÍ∞Ä Ìï®Ïàò: ÌòÑÏû¨Îäî MSE Í∏∞Î∞ò(Ïõê Î†àÌè¨ Ïä§ÌÉÄÏùº Ïú†ÏßÄ).
    """
    meta_learner.eval()
    
    support_x, support_y, query_x, query_y = test_task
    support_x, support_y = support_x.to(device), support_y.to(device)
    query_x, query_y = query_x.to(device), query_y.to(device)

    fmodel = copy.deepcopy(meta_learner)
    fmodel.to(device)
    
    inner_opt = torch.optim.SGD(fmodel.parameters(), lr=meta_learner.config['INNER_LR'])
    
    for step in range(num_adaptation_steps):
        inner_opt.zero_grad()
        fmodel.prior_net.train()
        outputs = fmodel.prior_net(support_x, sample=True)
        mse_loss = F.mse_loss(outputs, support_y)
        kl_loss = fmodel.prior_net.kl_divergence(meta_learner.prior_net)
        inner_loss = mse_loss + meta_learner.config['KL_WEIGHT'] * kl_loss

        if torch.isnan(inner_loss) or torch.isinf(inner_loss):
            print(f"!!! NaN or Inf detected at adaptation step {step+1}. Stopping evaluation. !!!")
            return float('nan'), None, None, None
            
        inner_loss.backward()
        inner_opt.step()
        
    fmodel.eval()
    with torch.no_grad():
        predictions = []
        for _ in range(num_eval_samples):
            pred = fmodel.prior_net(query_x, sample=True)
            pred = torch.nan_to_num(pred, nan=0.0, posinf=1e3, neginf=-1e3)
            predictions.append(pred.cpu())
        
        predictions_tensor = torch.stack(predictions)
        mean_prediction = predictions_tensor.mean(dim=0)
        std_prediction = predictions_tensor.std(dim=0)
        test_loss = F.mse_loss(mean_prediction, query_y.cpu())

    del fmodel
    torch.cuda.empty_cache()

    return test_loss.item(), mean_prediction, std_prediction, query_y.cpu()
